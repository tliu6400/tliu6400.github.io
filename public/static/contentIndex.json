{"musings/Reinforcement-learning-and-hyperparameters":{"slug":"musings/Reinforcement-learning-and-hyperparameters","filePath":"musings/Reinforcement learning and hyperparameters.md","title":"Reinforcement learning and hyperparameters","links":[],"tags":[],"content":""},"musings/Rockstars-and-superstars":{"slug":"musings/Rockstars-and-superstars","filePath":"musings/Rockstars and superstars.md","title":"Rockstars and superstars","links":["Radical-Candor"],"tags":[],"content":"I think this is a concept that I first read about in Radical Candor. It’s the idea that Rockstars and Superstars are two distinct personas. I think of myself more as an aspiring Superstar than Rockstar; it can be odd for me to see Rockstars be content with what they do. There are many super talented Rockstars, yet they don’t start their own companies, they seem like they “don’t have ambition.”"},"musings/Balancing-multi-reward-runs-is-short-sighted":{"slug":"musings/Balancing-multi-reward-runs-is-short-sighted","filePath":"musings/Balancing multi reward runs is short-sighted.md","title":"Balancing multi reward runs is short-sighted","links":[],"tags":[],"content":"Model merging seems more principled"},"musings/Building-research-infrastructure":{"slug":"musings/Building-research-infrastructure","filePath":"musings/Building research infrastructure.md","title":"Building research infrastructure","links":[],"tags":[],"content":"\nFood for thought\n\nhumer\nmedia-pipeline\ndataset API\n\n\n"},"musings/Cascading-systems-vs-SpeechLLMs":{"slug":"musings/Cascading-systems-vs-SpeechLLMs","filePath":"musings/Cascading systems vs SpeechLLMs.md","title":"Cascading systems vs SpeechLLMs","links":[],"tags":[],"content":"\nIf you have perfect data for every scenario (ex: two people talking to assistant), doesn’t matter what approach you do\n\nEverything in practice is conditioned on having incomplete data\n\n\nSpeechLLMs give you a “text handle” on everything if you pretrain on enough text-audio data, so you have hope for “generalization”\nCascading requires “feature engineering” or “hardcoding” (ex: we’ll provide transcript, prosody labels, etc.)\nTheoretically this hybrid approach, separate audio understanding and audio generation with text handles, could be interesting\nFrom a practical perspective though, cascading is easier to parallelize and evaluate\n\nReally hard for a small company to be really good at all of  audio understanding, audio generation, language modeling, long context, memory, etc.\nReally hard to balance improving one vs degrading another\n\n\n"},"musings/Enough-\"good\"-data-solves-everything":{"slug":"musings/Enough-\"good\"-data-solves-everything","filePath":"musings/Enough \"good\" data solves everything.md","title":"Enough \"good\" data solves everything","links":[],"tags":[],"content":""},"musings/Landscape-of-AI-companies-as-of-August-2025":{"slug":"musings/Landscape-of-AI-companies-as-of-August-2025","filePath":"musings/Landscape of AI companies as of August 2025.md","title":"Landscape of AI companies as of August 2025","links":[],"tags":[],"content":""},"musings/Looking-at-production-data":{"slug":"musings/Looking-at-production-data","filePath":"musings/Looking at production data.md","title":"Looking at production data","links":[],"tags":[],"content":""},"musings/Measuring-what's-actually-important":{"slug":"musings/Measuring-what's-actually-important","filePath":"musings/Measuring what's actually important.md","title":"Measuring what's actually important","links":[],"tags":[],"content":""},"musings/Multiple-rewards-vs-model-merging":{"slug":"musings/Multiple-rewards-vs-model-merging","filePath":"musings/Multiple rewards vs model merging.md","title":"Multiple rewards vs model merging","links":[],"tags":[],"content":""},"musings/Reinforcement-learning-and-entropy":{"slug":"musings/Reinforcement-learning-and-entropy","filePath":"musings/Reinforcement learning and entropy.md","title":"Reinforcement learning and entropy","links":[],"tags":[],"content":""},"musings/Reinforcement-learning-and-overoptimization":{"slug":"musings/Reinforcement-learning-and-overoptimization","filePath":"musings/Reinforcement learning and overoptimization.md","title":"Reinforcement learning and overoptimization","links":[],"tags":[],"content":""},"musings/Reinforcement-learning-is-a-zero-sum-game":{"slug":"musings/Reinforcement-learning-is-a-zero-sum-game","filePath":"musings/Reinforcement learning is a zero sum game.md","title":"Reinforcement learning is a zero sum game","links":["musings/Enough-\"good\"-data-solves-everything"],"tags":[],"content":"It’s kind of interesting the way people have been talking about RL recently, for example needing to “undo the RL” on the gpt-oss model. For LLMs, using RL to align the model to some objective also seems to corrupt the model in some other ways (either due to entropy collapse, overoptimization, or some other mechanism). It’s probably true that certain objectives corrupt the model is more important ways than others. This process feels zero sum compared to pretraining or SFT on good data (of course, Enough “good” data solves everything). This means RL is likely to be useful\nIt’s like playing whack-a-whole."},"musings/Reward-modeling-and-effectiveness-of-\"on-policy\"-preference":{"slug":"musings/Reward-modeling-and-effectiveness-of-\"on-policy\"-preference","filePath":"musings/Reward modeling and effectiveness of \"on-policy\" preference.md","title":"Reward modeling and effectiveness of \"on-policy\" preference","links":[],"tags":[],"content":"It’s intuitive that a reward model performs better when evaluating data that is in distribution to the data the reward model was trained on. Furthermore, when using this reward model as a reward signal during RL, the policy model is also able to climb this reward signal much faster; maybe because the learning signal is less noisy…\nAnother thing, it’s interesting that a preference model’s accuracy need only be ~60-70% to be “good enough”. That’s barely better than chance!"},"musings/What-do-pretraining,-finetuning,-and-reinforcement-learning-do":{"slug":"musings/What-do-pretraining,-finetuning,-and-reinforcement-learning-do","filePath":"musings/What do pretraining, finetuning, and reinforcement learning do?.md","title":"What do pretraining, finetuning, and reinforcement learning do?","links":["Scaling-Laws","musings/Enough-\"good\"-data-solves-everything","Reliability-RL","musings/Reinforcement-learning-and-entropy","Does-reinforcement-learning-learn-new-things","Learning-without-training-The-implicit-dynamics-of-in-context-learning","Pretraining-vs-SFT-vs-Reinforcement-Learning"],"tags":[],"content":"The common way of thinking about pretraining and finetuning is that they compress information from data into model parameters. In this sense, more data is better because it represents the true underlying data distribution better, and more parameters is better since it can meaningfully represent more information. Of course, people have spent a lot of time studying Scaling Laws to determine how to increases data and model parameters together.\nWe know Enough “good” data solves everything, but in practice our data is noisy, unwieldy, and not exactly in the shape that we want. This is probably why data labeling companies are so valuable. How do we get around this?\n\nFiltering\n\nOne thing we can do is filter out data that we think is too noisy, essentially masking out part of the true data distribution that we don’t care about learning how to model\nThe issue here is whether or not you can actually throw away learning this entire distribution. For example, if all noisy data is correlated with something you do actually care about performing inference on, then this won’t work\n\n\nConditioning\n\nAnother thing we can do is condition examples based on labels, this let’s the model still learn the noisy distribution, but to not use this distribution during inference\nOf course, during inference we sometimes want to use the “good conditioning” on some other inputs that are maybe highly correlated with “bad conditioning” during training, so when the model sees conflicting information like this, it may randomly decide whether or not to follow the conditioning. This is because our data is not perfect, and generalization is not perfect either\n\n\n\nReinforcement learning, on the other hand, does not really compress more information into the parameters, but rather “aligns” the model to behave in some certain way. In this sense, from my experience working on Reliability RL, RL seems to be like playing a zero sum game: you lose some capability to make some other capability more robust. It almost feels like quantum mechanics, the model parameters represent some quantum state (conditioned on “good conditioning” but bad inputs, holds a probability distribution on which direction to go), RL collapses this quantum state into an observation (specifically the one we want), but in the process the model loses it’s ability to represent more complex states. This is literally reflected by the relationship between Reinforcement learning and entropy.\nAside: This may be a controversial take; there’s a lot of open research about the “aha” moment of RL, whether the model is actually gaining information or if it’s just eliciting some information that isn’t easy to access through the model via prompting. See Does reinforcement learning learn new things. DSPy for example, is a framework for performing search through the prompt space, which may eventually reveal to a large extent, RL can be replaced by just using the right conditioning. There’s even a paper (Learning without training The implicit dynamics of in-context learning) that goes to say that any prompt can be represented as a weight update. It doesn’t go to say if the reverse is true, but if so, this has some big implications on the novelty of behaviors we see elicited through RL.\nTransclude of Pretraining-vs-SFT-vs-Reinforcement-Learning"},"index":{"slug":"index","filePath":"index.md","title":"Welcome to Quartz","links":[],"tags":[],"content":"This is a blank Quartz installation.\nSee the documentation for how to get started."}}